{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Approaches\n",
    "\n",
    "## Utilities and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.6.7\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print('Python Version: {}'.format( python_version() ) )\n",
    "\n",
    "# Utils\n",
    "from tqdm import tqdm_notebook # Progress bar\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "from   scipy.io import loadmat\n",
    "\n",
    "# sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors     import KNeighborsClassifier\n",
    "from sklearn.neighbors     import NearestNeighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation\n",
    "\n",
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (14096, 2048)\n",
      "Loading Training indexes : (7368,)\n",
      "Loading Query indexes : (1400,)\n",
      "Loading Gallery indexes : (5328,)\n"
     ]
    }
   ],
   "source": [
    "with open( \"PR_data/feature_data.json\", \"r\" ) as file:\n",
    "    features = json.load( file )\n",
    "    \n",
    "data = np.asarray( features )\n",
    "\n",
    "print( 'Data shape: {}'.format( data.shape ) )\n",
    "\n",
    "# Load matfile\n",
    "mat = loadmat( 'PR_data/cuhk03_new_protocol_config_labeled.mat' )\n",
    "\n",
    "# Load labels\n",
    "labels = mat[ 'labels' ].flatten()\n",
    "\n",
    "# Load camId\n",
    "camIds = mat[ 'camId' ].flatten()\n",
    "\n",
    "# Load indexes\n",
    "train_idxs   = mat[ 'train_idx' ].flatten()\n",
    "query_idxs    = mat[ 'query_idx' ].flatten()\n",
    "gallery_idxs = mat[ 'gallery_idx' ].flatten()\n",
    "\n",
    "# Load training indexes\n",
    "print( \"Loading Training indexes : {}\".format( train_idxs.shape ) )\n",
    "print( \"Loading Query indexes : {}\".format( query_idxs.shape ) )\n",
    "print( \"Loading Gallery indexes : {}\".format( gallery_idxs.shape ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train/Query/Gallery Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set : (7368, 2048)\n",
      "Train Label : (7368,)\n",
      "\n",
      "Query Set : (1400, 2048)\n",
      "Query Label : (1400,)\n",
      "Query CamId : (1400,)\n",
      "\n",
      "Gallery Set : (5328, 2048)\n",
      "Gallery Label : (5328,)\n",
      "Gallery CamId : (5328,)\n"
     ]
    }
   ],
   "source": [
    "def generateSets():\n",
    "    # Create Train Set\n",
    "    train_set   = []\n",
    "    train_label = []\n",
    "\n",
    "    for i in train_idxs:\n",
    "        train_set.append( data[ i - 1 ] )\n",
    "        train_label.append( labels[ i - 1 ] )\n",
    "\n",
    "    train_set   = np.asarray( train_set )\n",
    "    train_label = np.asarray( train_label )\n",
    "\n",
    "    print( 'Train Set : {}'.format( train_set.shape ) )\n",
    "    print( 'Train Label : {}'.format( train_label.shape ) ) \n",
    "\n",
    "    # Create Query Set\n",
    "    query_set   = []\n",
    "    query_label = []\n",
    "    query_camId = []\n",
    "\n",
    "    for i in query_idxs:\n",
    "        query_set.append( data[ i - 1] )\n",
    "        query_label.append( labels[ i - 1 ] )\n",
    "        query_camId.append( camIds[ i - 1 ] )\n",
    "\n",
    "    query_set   = np.asarray( query_set )\n",
    "    query_label = np.asarray( query_label )\n",
    "    query_camId = np.asarray( query_camId )\n",
    "\n",
    "    print( '\\nQuery Set : {}'.format( query_set.shape ) )\n",
    "    print( 'Query Label : {}'.format( query_label.shape ) )\n",
    "    print( 'Query CamId : {}'.format( query_camId.shape ) )\n",
    "\n",
    "\n",
    "    # Create Gallery Set\n",
    "    gallery_set   = []\n",
    "    gallery_label = []\n",
    "    gallery_camId = []\n",
    "\n",
    "    for i in gallery_idxs:\n",
    "        gallery_set.append( data[ i - 1] )\n",
    "        gallery_label.append( labels[ i - 1 ] )\n",
    "        gallery_camId.append( camIds[ i - 1 ] )\n",
    "\n",
    "    gallery_set   = np.asarray( gallery_set )\n",
    "    gallery_label = np.asarray( gallery_label )\n",
    "    gallery_camId = np.asarray( gallery_camId )\n",
    "\n",
    "    print( '\\nGallery Set : {}'.format( gallery_set.shape ) )\n",
    "    print( 'Gallery Label : {}'.format( gallery_label.shape ) )\n",
    "    print( 'Gallery CamId : {}'.format( gallery_camId.shape ) )\n",
    "    \n",
    "    return train_set, train_label, query_set, query_label, query_camId, gallery_set, gallery_label, gallery_camId\n",
    "    \n",
    "train_set, train_label, query_set, query_label, query_camId, gallery_set, gallery_label, gallery_camId = generateSets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "* Pick 100 random identites from the training set\n",
    "* Remove all data with those 100 identities from training set and put in validation set\n",
    "\n",
    "There exists an idea in Computer Vision that you use validation set only to specify the number of iterations that is optimal for your design and then you include your validation set into train set and perform final training (without validation set) for this fixed amount of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting *100* Random Identities For Validation Set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d48edfd5f44c79a05430687f0a6419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set with Validation removed: (6399, 2049)\n",
      "Validation Set: (970, 2049) ( Has an extra row due to np.zeros )\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "train_unique_labels = np.unique( train_label )\n",
    "\n",
    "# Select 100 Random Identities\n",
    "shuffled_validation_labels = shuffle( train_unique_labels, random_state = RANDOM_STATE )[ : 100 ] \n",
    "print( 'Selecting *{}* Random Identities For Validation Set'.format( shuffled_validation_labels.shape[ 0 ] ) )\n",
    "\n",
    "train_validate = np.zeros( ( 1, 2049 ) )\n",
    "train_set_validate_removed = np.vstack( ( train_set.T, train_label ) ).T\n",
    "\n",
    "for identity in tqdm_notebook( shuffled_validation_labels ):\n",
    "    \n",
    "    # Go through data and remove rows with that identity\n",
    "    validation = train_set_validate_removed[ np.where( train_set_validate_removed[ :, -1 ] == identity ) ]\n",
    "    \n",
    "    train_validate = np.vstack( ( train_validate, validation ) )\n",
    "    \n",
    "    train_set_validate_removed = train_set_validate_removed[ np.where( train_set_validate_removed[ :, -1 ] != identity )]\n",
    "    \n",
    "print( 'Training Set with Validation removed: {}'.format( train_set_validate_removed.shape ) )\n",
    "print( 'Validation Set: {} ( Has an extra row due to np.zeros )'.format( train_validate.shape ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Train Set: (6399, 2048) \n",
      "Train Labels: (6399, 1)\n",
      "CV Validation Set: (969, 2048) \n",
      "Validation Labels: (969, 1)\n"
     ]
    }
   ],
   "source": [
    "cv_train_set   = train_set_validate_removed.T[ : -1 ].T\n",
    "cv_train_label = train_set_validate_removed.T[ -1 : ].T\n",
    "\n",
    "cv_validation_set = train_validate.T[ : -1 ].T[ 1: ]\n",
    "cv_validation_label = train_validate.T[ -1 : ].T[ 1 : ]\n",
    "\n",
    "print( 'CV Train Set: {} \\nTrain Labels: {}'.format( cv_train_set.shape, cv_train_label.shape ) )\n",
    "print( 'CV Validation Set: {} \\nValidation Labels: {}'.format( cv_validation_set.shape, cv_validation_label.shape ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Baseline Approach\n",
    "\n",
    "### Create Augmented Query And Gallery Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Augmented: (1400, 2050)\n",
      "Gallery Augmented: (5328, 2050)\n"
     ]
    }
   ],
   "source": [
    "# Query Augmented\n",
    "qs = query_set.T\n",
    "\n",
    "query_augmented = np.vstack( ( qs, query_camId, query_label ) )\n",
    "query_augmented = query_augmented.T\n",
    "\n",
    "# Gallery Augmented\n",
    "gs = gallery_set.T\n",
    "\n",
    "gallery_augmented = np.vstack( ( gs, gallery_camId, gallery_label ) )\n",
    "gallery_augmented = gallery_augmented.T\n",
    "\n",
    "print( 'Query Augmented: {}'.format( query_augmented.shape ) )\n",
    "print( 'Gallery Augmented: {}'.format( gallery_augmented.shape ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Baseline Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77cc2a565124429aefe7dcb2b54a59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1400), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# KNN Parameters\n",
    "knn_n_neighbors = 20\n",
    "knn_metric = 'euclidean'\n",
    "\n",
    "KNN = NearestNeighbors( n_neighbors = knn_n_neighbors, metric = knn_metric )\n",
    "KNN.fit( gallery_augmented[ :, : -2 ], gallery_augmented[ :, -1 : ] )\n",
    "\n",
    "query_rank_list = []\n",
    "query_indices = []\n",
    "\n",
    "# for i in range( 2,3 ):\n",
    "for i in tqdm_notebook( range( query_augmented.shape[ 0 ] ) ):\n",
    "\n",
    "    \n",
    "    query_label = query_augmented[ i, -1 ].astype( int )\n",
    "\n",
    "    # Test query point\n",
    "    X_test = query_augmented[ i ][ : -2 ].reshape( 1, -1 ) # Remove last 2 columns ( camId and label )\n",
    "    \n",
    "    distances, indices = KNN.kneighbors( X_test ) # Neighbours are ordered closest to furthest\n",
    "    \n",
    "    # Compare\n",
    "    distances = distances.flatten()\n",
    "    indices   = indices.flatten()\n",
    "    \n",
    "    removed_indices = []\n",
    "    \n",
    "    # Remove indices with same camId and Row\n",
    "    for ind in indices:\n",
    "        if( ~( gallery_augmented[ ind, -1 ] == query_label and \n",
    "           gallery_augmented[ ind, -2 ] == query_augmented[ i, -2 ].astype( int ) ) ):\n",
    "            \n",
    "            removed_indices.append( ind )\n",
    "    \n",
    "    removed_indices = np.asarray( removed_indices )\n",
    "    \n",
    "    query_indices.append( removed_indices )\n",
    "    \n",
    "    rank_list = [ gallery_augmented[ ind, -1 ].astype( int ) == query_label for ind in removed_indices[ : 10 ] ]\n",
    "    query_rank_list.append( rank_list )\n",
    "    \n",
    "query_rank_list = np.asarray( query_rank_list )\n",
    "query_indices = np.asarray( query_indices )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Rank List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_rank_list = query_rank_list.astype( int )\n",
    "\n",
    "# Save as CSV so we dont have to fucking run that 20 minute code anymore\n",
    "np.savetxt( 'query_rank_list.csv', query_rank_list, fmt='%i', delimiter= ',' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Saved Rank List\n",
    "* This avoids us having to rerun the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv file\n",
    "query_rank_list = np.loadtxt( 'query_rank_list.csv', delimiter = ',' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbor Baseline Evaluation\n",
    "\n",
    "#### Rank@ Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank@1: 47.0%\n",
      "rank@5: 66.85714285714286%\n",
      "rank@10: 74.92857142857143%\n"
     ]
    }
   ],
   "source": [
    "rankAt1  = query_rank_list.T[ 0 ]\n",
    "rankAt5  = query_rank_list.T[ : 5 ].T\n",
    "rankAt10 = query_rank_list.T[ : 10 ].T\n",
    "\n",
    "cmc1  = rankAt1\n",
    "cmc5  = np.sum( rankAt5, axis = 1 ) > 0 \n",
    "cmc10 = np.sum( rankAt10, axis = 1 ) > 0\n",
    "\n",
    "print( 'rank@1: {}%'.format( np.sum( cmc1 ) / cmc1.shape[ 0 ] * 100 ) )\n",
    "print( 'rank@5: {}%'.format( np.sum( cmc5 ) / cmc5.shape[ 0 ] * 100 ) )\n",
    "print( 'rank@10: {}%'.format( np.sum( cmc10 ) / cmc10.shape[ 0 ] * 100 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mAP Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731cbb3797e245c8bc0fc41ec3699812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1400), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mAP: 0.5169191146155432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aufar/py_36_env/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "average_precision=[]\n",
    "for i in tqdm_notebook(range(len(query_rank_list))):\n",
    "    Precision=[]\n",
    "    Recall=[]\n",
    "    Total=sum(query_rank_list[i,:])\n",
    "    for j in range(1,query_rank_list.shape[1]+1):\n",
    "        Precision.append(sum(query_rank_list[i,:j]/j))\n",
    "        Recall.append(sum(query_rank_list[i,:j]/Total))\n",
    "        if Recall[j-1] == 1:\n",
    "            break\n",
    "\n",
    "    indices=[]\n",
    "    Recall=np.array(Recall)\n",
    "    Recall = np.nan_to_num(Recall)\n",
    "    Precision=np.array(Precision)\n",
    "\n",
    "    #re-map the recall and position to 0.1 ... 1.0\n",
    "    recall = np.arange(0.1,1.1,0.1)\n",
    "    recall = np.asarray(recall)\n",
    "    precision=[]\n",
    "\n",
    "    for j in range(recall.size):\n",
    "        for i in range(len(Recall)):\n",
    "            if Recall[i] >= recall[j]:\n",
    "                #interprelation of the data, finding the map between now and future\n",
    "                precision.append(max(Precision[Recall.tolist().index(Recall[i]):len(Precision)]))\n",
    "                break\n",
    "    if np.sum(precision) == 0:\n",
    "        average_precision.append(0)\n",
    "    else:\n",
    "        average_precision.append((np.sum(precision)+precision[0])/11)\n",
    "\n",
    "average_precision = np.nan_to_num(average_precision)\n",
    "print( 'mAP: {}'.format( np.mean(average_precision) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Means Clustering Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set : (7368, 2048)\n",
      "Train Label : (7368,)\n",
      "\n",
      "Query Set : (1400, 2048)\n",
      "Query Label : (1400,)\n",
      "Query CamId : (1400,)\n",
      "\n",
      "Gallery Set : (5328, 2048)\n",
      "Gallery Label : (5328,)\n",
      "Gallery CamId : (5328,)\n"
     ]
    }
   ],
   "source": [
    "train_set, train_label, query_set, query_label, query_camId, gallery_set, gallery_label, gallery_camId = generateSets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5328\n",
      "700.0\n",
      "2048\n",
      "1400\n",
      "[3060 4419]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d0b4205f9248d0b6c791328318def1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1400), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.5247208931419458\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "#size(n_samples, n_features)\n",
    "kmeans = KMeans(n_clusters=np.size(np.unique(query_label)), random_state=0).fit(gallery_set)\n",
    "kmeans.cluster_centers_\n",
    "kmeans.predict\n",
    "print(np.size(kmeans.labels_))\n",
    "print(np.size(kmeans.cluster_centers_)/2048)\n",
    "print(np.size(kmeans.cluster_centers_[110]))\n",
    "predicted_class = kmeans.predict(query_set)\n",
    "print(predicted_class.size)\n",
    "# kmeans.labels_.tolist().index(101)\n",
    "searchval = 0\n",
    "ii = np.where(kmeans.labels_ == searchval)[0]\n",
    "print(ii)\n",
    "\n",
    "\n",
    "predicted_class = kmeans.predict(query_set)\n",
    "# Query Augmented\n",
    "qs = predicted_class.T\n",
    "query_augmented = np.vstack( ( qs, query_camId, query_label ) )\n",
    "query_augmented = query_augmented.T\n",
    "\n",
    "qs = kmeans.labels_.T\n",
    "gallery_augmented = np.vstack( ( qs, gallery_camId, gallery_label ) )\n",
    "gallery_augmented = gallery_augmented.T\n",
    "\n",
    "query_rank_list_kmean = []\n",
    "\n",
    "# for i in range( 2,4 ):\n",
    "for i in tqdm_notebook(range(predicted_class.shape[0])): \n",
    "    \n",
    "    ii = np.where(kmeans.labels_ == predicted_class[i])[0]\n",
    "    for j in ii:\n",
    "        if (gallery_augmented[j,1] != query_augmented[i,1]):\n",
    "            query_rank_list_kmean.append(gallery_augmented[j,2] == query_augmented[i,2])  \n",
    "    \n",
    "query_rank_list_kmean = np.asarray( query_rank_list_kmean )\n",
    "\n",
    "print(sum(query_rank_list_kmean)/query_rank_list_kmean.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
